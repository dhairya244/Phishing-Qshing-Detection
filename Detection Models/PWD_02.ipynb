{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S243MgYoj6zx"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import re\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load datasets\n",
        "legitimate_df = pd.read_csv('structured_data_legitimate.csv')\n",
        "phishing_df = pd.read_csv('structured_data_phishing.csv')"
      ],
      "metadata": {
        "id": "k5dvbCYmkFkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Combine datasets\n",
        "# Assuming 'label' column contains 0 for legitimate and 1 for phishing\n",
        "legitimate_df['label'] = 0\n",
        "phishing_df['label'] = 1\n",
        "df = pd.concat([legitimate_df, phishing_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "T_aK8u0mkLbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature Engineering (extract features from URL)\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Extracting basic domain and path features\n",
        "    domain = urlparse(url).hostname\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    # Extracting URL length\n",
        "    features['url_length'] = len(url)\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['num_subdomains'] = domain.count('.') - 1  # Subdomains are separated by dots\n",
        "\n",
        "    # Checking for HTTPS\n",
        "    features['uses_https'] = url.startswith('https')\n",
        "\n",
        "    # Presence of IP address in the URL\n",
        "    features['has_ip'] = bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url))\n",
        "\n",
        "    # Presence of special characters\n",
        "    features['has_special_chars'] = bool(re.search(r'[^A-Za-z0-9./:_-]', url))\n",
        "\n",
        "    # Query and fragment analysis (ignore query and fragment for simplicity)\n",
        "    query_present = bool(urlparse(url).query)\n",
        "    fragment_present = bool(urlparse(url).fragment)\n",
        "\n",
        "    features['has_query'] = query_present\n",
        "    features['has_fragment'] = fragment_present\n",
        "\n",
        "    # Check if domain is from a known legitimate source (e.g., Google, Facebook)\n",
        "    known_domains = ['google.com', 'facebook.com', 'twitter.com', 'amazon.com']\n",
        "    features['is_known_domain'] = int(any(domain.endswith(known_domain) for known_domain in known_domains))\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "-hBWuShHkXbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the URL feature extraction to all URLs\n",
        "df_features = df['URL'].apply(extract_features)\n",
        "\n",
        "# Convert feature dictionaries into a DataFrame\n",
        "features_df = pd.DataFrame(df_features.tolist())"
      ],
      "metadata": {
        "id": "bMlXZ96Nkb8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Prepare the final feature set and labels\n",
        "X = features_df  # Feature matrix\n",
        "y = df['label']  # Target vector (0 for legitimate, 1 for phishing)"
      ],
      "metadata": {
        "id": "aKm8Dm0qkdRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Data Scaling (important for models like SVM, KNN, etc.)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "tqswL0OUkgnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Train a Random Forest model (or you can choose other classifiers)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "KfjdA5-Skm0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Classification Report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Step 10: Cross-validation score\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f'\\nCross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVCx1ThPkoPN",
        "outputId": "0d595b25-04e9-4a25-cedb-bfc513f107e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.74%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      3265\n",
            "           1       1.00      0.99      1.00      2052\n",
            "\n",
            "    accuracy                           1.00      5317\n",
            "   macro avg       1.00      1.00      1.00      5317\n",
            "weighted avg       1.00      1.00      1.00      5317\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3265    0]\n",
            " [  14 2038]]\n",
            "\n",
            "Cross-Validation Accuracy: 99.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Dynamic Prediction Function\n",
        "def dynamic_predict_url():\n",
        "    print(\"Welcome to the URL phishing detection system.\")\n",
        "    print(\"You can enter a URL to check if it is phishing or legitimate.\")\n",
        "    print(\"To exit, type 'exit'.\")\n",
        "\n",
        "    while True:\n",
        "        # Get the URL input from the user\n",
        "        url = input(\"Enter URL: \")\n",
        "\n",
        "        # Exit condition\n",
        "        if url.lower() == 'exit':\n",
        "            print(\"Exiting the prediction system.\")\n",
        "            break\n",
        "\n",
        "        # Extract features from the URL\n",
        "        features = extract_features(url)\n",
        "        features_df = pd.DataFrame([features])\n",
        "\n",
        "        # Scale the features using the scaler\n",
        "        features_scaled = scaler.transform(features_df)\n",
        "\n",
        "        # Predict using the trained model\n",
        "        prediction = model.predict(features_scaled)\n",
        "\n",
        "        # Display the result\n",
        "        if prediction == 1:\n",
        "            print(f\"URL: {url} -> Phishing\")\n",
        "        else:\n",
        "            print(f\"URL: {url} -> Legitimate\")\n",
        "\n",
        "# Call the dynamic prediction function\n",
        "dynamic_predict_url()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEnO5BZ2kuFC",
        "outputId": "4730217d-0c72-4f1b-b515-0d90a785e490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the URL phishing detection system.\n",
            "You can enter a URL to check if it is phishing or legitimate.\n",
            "To exit, type 'exit'.\n",
            "Enter URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=pEnO5BZ2kuFC\n",
            "URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=pEnO5BZ2kuFC -> Phishing\n",
            "Enter URL: exit\n",
            "Exiting the prediction system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach 02**"
      ],
      "metadata": {
        "id": "yvKVfNeFm--f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Step 1: Load datasets\n",
        "legitimate_df = pd.read_csv('structured_data_legitimate.csv')\n",
        "phishing_df = pd.read_csv('structured_data_phishing.csv')"
      ],
      "metadata": {
        "id": "wACYRUk3m-UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Combine datasets\n",
        "legitimate_df['label'] = 0  # Label 0 for legitimate sites\n",
        "phishing_df['label'] = 1  # Label 1 for phishing sites\n",
        "\n",
        "# Concatenate the datasets\n",
        "df = pd.concat([legitimate_df, phishing_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "5MIV-YJDnKiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Feature Extraction Function\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Extract domain and path\n",
        "    domain = urlparse(url).hostname\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    # URL length\n",
        "    features['url_length'] = len(url)\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['num_subdomains'] = domain.count('.') - 1\n",
        "\n",
        "    # Presence of HTTPS\n",
        "    features['uses_https'] = url.startswith('https')\n",
        "\n",
        "    # Presence of an IP address in the URL\n",
        "    features['has_ip'] = bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url))\n",
        "\n",
        "    # Presence of special characters\n",
        "    features['has_special_chars'] = bool(re.search(r'[^A-Za-z0-9./:_-]', url))\n",
        "\n",
        "    # Query and fragment analysis\n",
        "    features['has_query'] = bool(urlparse(url).query)\n",
        "    features['has_fragment'] = bool(urlparse(url).fragment)\n",
        "\n",
        "    # Check for suspicious domain (random characters in domain name)\n",
        "    features['has_random_chars'] = bool(re.search(r'\\.[a-z]{5,}', domain))\n",
        "\n",
        "    # Check if the domain is from a known legitimate source\n",
        "    known_domains = ['google.com', 'facebook.com', 'youtube.com', 'amazon.com', 'colab.research.google.com']\n",
        "    features['is_known_domain'] = int(any(domain.endswith(known_domain) for known_domain in known_domains))\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "mhPtW3fRnPr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Apply feature extraction on all URLs\n",
        "df_features = df['URL'].apply(extract_features)\n",
        "\n",
        "# Step 5: Convert the extracted features into a DataFrame\n",
        "features_df = pd.DataFrame(df_features.tolist())\n",
        "\n",
        "# Step 6: Prepare the feature matrix (X) and target vector (y)\n",
        "X = features_df  # Features\n",
        "y = df['label']  # Target labels (0 = legitimate, 1 = phishing)\n",
        "\n",
        "# Step 7: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Feature scaling (important for models that use distance metrics)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 9: Train Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 10: Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Classification Report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Step 11: Cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f'\\nCross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbTt_qOinlp_",
        "outputId": "aa86f34e-befa-4a64-d5d7-bb6b73e15e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.83%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      3265\n",
            "           1       1.00      1.00      1.00      2052\n",
            "\n",
            "    accuracy                           1.00      5317\n",
            "   macro avg       1.00      1.00      1.00      5317\n",
            "weighted avg       1.00      1.00      1.00      5317\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3265    0]\n",
            " [   9 2043]]\n",
            "\n",
            "Cross-Validation Accuracy: 99.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Dynamic URL Prediction\n",
        "def dynamic_predict_url():\n",
        "    print(\"Welcome to the URL phishing detection system.\")\n",
        "    print(\"You can enter a URL to check if it is phishing or legitimate.\")\n",
        "    print(\"To exit, type 'exit'.\")\n",
        "\n",
        "    while True:\n",
        "        # Get the URL input from the user\n",
        "        url = input(\"Enter URL: \")\n",
        "\n",
        "        # Exit condition\n",
        "        if url.lower() == 'exit':\n",
        "            print(\"Exiting the prediction system.\")\n",
        "            break\n",
        "\n",
        "        # Extract features from the URL\n",
        "        features = extract_features(url)\n",
        "        features_df = pd.DataFrame([features])\n",
        "\n",
        "        # Scale the features using the scaler\n",
        "        features_scaled = scaler.transform(features_df)\n",
        "\n",
        "        # Predict using the trained model\n",
        "        prediction = model.predict(features_scaled)\n",
        "\n",
        "        # Display the result\n",
        "        if prediction == 1:\n",
        "            print(f\"URL: {url} -> Phishing\")\n",
        "        else:\n",
        "            print(f\"URL: {url} -> Legitimate\")\n",
        "\n",
        "# Call the dynamic prediction function (uncomment the line below to run in interactive mode)\n",
        "dynamic_predict_url()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cRTWF3dnprv",
        "outputId": "4a9d2c91-6579-41ce-d20d-8f797491ccd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the URL phishing detection system.\n",
            "You can enter a URL to check if it is phishing or legitimate.\n",
            "To exit, type 'exit'.\n",
            "Enter URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=4cRTWF3dnprv\n",
            "URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=4cRTWF3dnprv -> Phishing\n",
            "Enter URL: exit\n",
            "Exiting the prediction system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tldextract python-whois\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7DC91ccFbTT",
        "outputId": "9ee0d5e9-194c-43e3-88aa-557d12c3ea7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.3)\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.32.3)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.16.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading python_whois-0.9.5-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-whois\n",
            "Successfully installed python-whois-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import tldextract\n",
        "from urllib.parse import urlparse\n",
        "from math import log2\n",
        "from datetime import datetime\n",
        "import whois\n",
        "import ssl\n",
        "import socket\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('combined_data_randomized.csv')\n",
        "\n",
        "# Define helper functions for feature extraction\n",
        "\n",
        "# URL-Based Features\n",
        "def calculate_entropy(url):\n",
        "    \"\"\"Calculate Shannon entropy of a URL.\"\"\"\n",
        "    probabilities = [float(url.count(c)) / len(url) for c in set(url)]\n",
        "    return -sum(p * log2(p) for p in probabilities if p > 0)\n",
        "\n",
        "def contains_ip(url):\n",
        "    \"\"\"Check if the URL contains an IP address.\"\"\"\n",
        "    return 1 if re.search(r'\\b\\d{1,3}(\\.\\d{1,3}){3}\\b', url) else 0\n",
        "\n",
        "def count_special_chars(url):\n",
        "    \"\"\"Count special characters in the URL.\"\"\"\n",
        "    return sum(1 for char in url if char in ['@', '%', '#', '&', '?', '='])\n",
        "\n",
        "def check_suspicious_tld(url):\n",
        "    \"\"\"Check if the URL has a suspicious TLD.\"\"\"\n",
        "    suspicious_tlds = ['.xyz', '.top', '.info', '.tk', '.ml']  # Example list\n",
        "    ext = tldextract.extract(url)\n",
        "    return 1 if f\".{ext.suffix}\" in suspicious_tlds else 0\n",
        "\n",
        "# Domain WHOIS-Based Features\n",
        "def get_domain_info(url):\n",
        "    \"\"\"Get domain age and time to expiration using WHOIS.\"\"\"\n",
        "    try:\n",
        "        domain = tldextract.extract(url).registered_domain\n",
        "        w = whois.whois(domain)\n",
        "        if isinstance(w.creation_date, list):\n",
        "            creation_date = w.creation_date[0]\n",
        "        else:\n",
        "            creation_date = w.creation_date\n",
        "        if isinstance(w.expiration_date, list):\n",
        "            expiration_date = w.expiration_date[0]\n",
        "        else:\n",
        "            expiration_date = w.expiration_date\n",
        "        domain_age = (datetime.now() - creation_date).days if creation_date else None\n",
        "        time_to_expire = (expiration_date - datetime.now()).days if expiration_date else None\n",
        "        return domain_age, time_to_expire\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def check_whois_privacy(url):\n",
        "    \"\"\"Check if WHOIS privacy protection is enabled.\"\"\"\n",
        "    try:\n",
        "        domain = tldextract.extract(url).registered_domain\n",
        "        w = whois.whois(domain)\n",
        "        return 1 if w.privacy else 0\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_registrar_info(url):\n",
        "    \"\"\"Get domain registrar information.\"\"\"\n",
        "    try:\n",
        "        domain = tldextract.extract(url).registered_domain\n",
        "        w = whois.whois(domain)\n",
        "        return w.registrar\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# SSL Certificate-Based Features\n",
        "def get_ssl_info(url):\n",
        "    \"\"\"Get SSL certificate validity period and issuer.\"\"\"\n",
        "    try:\n",
        "        hostname = urlparse(url).hostname\n",
        "        ctx = ssl.create_default_context()\n",
        "        with ctx.wrap_socket(socket.socket(), server_hostname=hostname) as s:\n",
        "            s.connect((hostname, 443))\n",
        "            cert = s.getpeercert()\n",
        "            issuer = dict(x[0] for x in cert['issuer'])\n",
        "            validity_period = (datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y GMT') -\n",
        "                               datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y GMT')).days\n",
        "            issuer_name = issuer.get('organizationName', 'Unknown')\n",
        "            return validity_period, issuer_name\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "# Extract URL-Based Features\n",
        "data['URL Length'] = data['URL'].apply(len)\n",
        "data['URL Entropy'] = data['URL'].apply(calculate_entropy)\n",
        "data['Contains IP'] = data['URL'].apply(contains_ip)\n",
        "data['Number of Dots'] = data['URL'].apply(lambda x: x.count('.'))\n",
        "data['Contains Hyphen'] = data['URL'].apply(lambda x: 1 if '-' in urlparse(x).netloc else 0)\n",
        "data['Special Characters Count'] = data['URL'].apply(count_special_chars)\n",
        "data['Suspicious TLD'] = data['URL'].apply(check_suspicious_tld)\n",
        "\n",
        "# Extract Domain WHOIS-Based Features\n",
        "data[['Domain Age', 'Time to Expire']] = data['URL'].apply(\n",
        "    lambda x: pd.Series(get_domain_info(x))\n",
        ")\n",
        "data['WHOIS Privacy'] = data['URL'].apply(check_whois_privacy)\n",
        "data['Registrar'] = data['URL'].apply(get_registrar_info)\n",
        "\n",
        "# Extract SSL Certificate-Based Features\n",
        "data[['SSL Validity Period', 'SSL Issuer']] = data['URL'].apply(\n",
        "    lambda x: pd.Series(get_ssl_info(x))\n",
        ")\n",
        "\n",
        "# Save the dataset with extracted features\n",
        "data.to_csv('new_dataset_with_features.csv', index=False)\n",
        "\n",
        "# Optionally preview the dataset\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "XgF1JL1WFYcf",
        "outputId": "32736295-dc00-4c86-bea0-d393aac3abc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'whois'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a2f5d96d18e7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whois'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original dataset\n",
        "file_path = 'combined_data_randomized.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Create a new dataframe with only the 'URL' and 'label' columns\n",
        "new_df = df[['URL', 'label']]\n",
        "\n",
        "# Save the new dataframe to a CSV file\n",
        "new_file_path = 'filtered_data.csv'\n",
        "new_df.to_csv(new_file_path, index=False)\n",
        "\n",
        "print(f\"New dataset saved as: {new_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snlFfq_-ZvKO",
        "outputId": "e97fb3e2-e88d-40d1-c402-04e65999d514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset saved as: filtered_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tldextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWfdGUjsdY7y",
        "outputId": "80afcb80-aa06-4d6e-8df5-091e64102153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.8.30)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find the total number of records in filtered_data.csv\n",
        "\n",
        "# Load the filtered dataset\n",
        "filtered_df = pd.read_csv('filtered_data.csv')\n",
        "\n",
        "# Get the total number of records\n",
        "total_records = len(filtered_df)\n",
        "\n",
        "# Print the total number of records\n",
        "print(f\"Total number of records in filtered_data.csv: {total_records}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8liIDpIgJkJ",
        "outputId": "2b5bb14e-2c53-4db0-a5b9-1a91f67aaf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records in filtered_data.csv: 26585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tldextract\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import ssl\n",
        "import socket\n",
        "\n",
        "# Function to extract URL length\n",
        "def get_url_length(url):\n",
        "    return len(url)\n",
        "\n",
        "# Function to extract the number of subdomains\n",
        "def get_num_subdomains(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return len(ext.subdomain.split('.')) if ext.subdomain else 0\n",
        "\n",
        "# Function to check if URL uses HTTPS\n",
        "def is_https(url):\n",
        "    return 1 if urlparse(url).scheme == \"https\" else 0\n",
        "\n",
        "# Function to extract suspicious keywords in the URL\n",
        "def contains_suspicious_keywords(url):\n",
        "    suspicious_keywords = ['login', 'verify', 'account', 'update', 'secure', 'banking', 'payment']\n",
        "    return any(keyword in url for keyword in suspicious_keywords)\n",
        "\n",
        "# Function to check for IP address in the URL\n",
        "def contains_ip_address(url):\n",
        "    pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    return 1 if re.search(pattern, url) else 0\n",
        "\n",
        "# Function to extract the number of hyphens in the domain name\n",
        "def num_hyphens_in_domain(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return ext.domain.count('-')\n",
        "\n",
        "# Function to count dots in the URL\n",
        "def count_dots(url):\n",
        "    return url.count('.')\n",
        "\n",
        "# Function to check if the URL contains a shortened link (bit.ly, goo.gl, etc.)\n",
        "def is_url_shortened(url):\n",
        "    shortened_services = ['bit.ly', 'goo.gl', 'tinyurl.com']\n",
        "    return 1 if any(service in url for service in shortened_services) else 0\n",
        "\n",
        "# Function to check if URL contains the \"@\" symbol\n",
        "def contains_at_symbol(url):\n",
        "    return 1 if '@' in url else 0\n",
        "\n",
        "# Function to check if the SSL certificate is valid (optional, may require network requests)\n",
        "def ssl_cert_valid(url):\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        hostname = parsed_url.hostname\n",
        "        context = ssl.create_default_context()\n",
        "        connection = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=hostname)\n",
        "        connection.connect((hostname, 443))\n",
        "        cert = connection.getpeercert()\n",
        "        return 1 if cert else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Function to extract the TLD (Top-Level Domain)\n",
        "def get_tld(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return ext.suffix\n",
        "\n",
        "# Function to extract the number of special characters in the URL\n",
        "def count_special_characters(url):\n",
        "    return len(re.findall(r'[^A-Za-z0-9]', url))\n",
        "\n",
        "# Function to extract URL features\n",
        "def extract_url_features(url):\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"url_length\": get_url_length(url),\n",
        "        \"num_subdomains\": get_num_subdomains(url),\n",
        "        \"is_https\": is_https(url),\n",
        "        \"contains_suspicious_keywords\": contains_suspicious_keywords(url),\n",
        "        \"contains_ip_address\": contains_ip_address(url),\n",
        "        \"num_hyphens_in_domain\": num_hyphens_in_domain(url),\n",
        "        \"dot_count\": count_dots(url),\n",
        "        \"is_url_shortened\": is_url_shortened(url),\n",
        "        \"contains_at_symbol\": contains_at_symbol(url),\n",
        "        \"ssl_cert_valid\": ssl_cert_valid(url),\n",
        "        \"tld\": get_tld(url),\n",
        "        \"special_char_count\": count_special_characters(url)\n",
        "    }\n",
        "\n",
        "# Load the filtered dataset with URL and label\n",
        "file_path = 'filtered_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Analyze only the first 200 URLs\n",
        "df_subset = df.head(26585)\n",
        "\n",
        "# Extract features from URLs\n",
        "url_features = df_subset['URL'].apply(extract_url_features)\n",
        "\n",
        "# Convert the features into a dataframe\n",
        "features_df = pd.DataFrame(url_features.tolist())\n",
        "\n",
        "# Add the label column to the features dataframe\n",
        "features_df['label'] = df_subset['label']\n",
        "\n",
        "# Save the new dataset with extracted features into a CSV file\n",
        "output_path = 'extracted_url_features_26585.csv'\n",
        "features_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Extracted URL features for first 200 URLs saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "jWArxYBXFBNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tldextract\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import ssl\n",
        "import socket\n",
        "\n",
        "# Function to extract URL length\n",
        "def get_url_length(url):\n",
        "    return len(url)\n",
        "\n",
        "# Function to extract the number of subdomains\n",
        "def get_num_subdomains(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return len(ext.subdomain.split('.')) if ext.subdomain else 0\n",
        "\n",
        "# Function to check if URL uses HTTPS\n",
        "def is_https(url):\n",
        "    return 1 if urlparse(url).scheme == \"https\" else 0\n",
        "\n",
        "# Function to extract suspicious keywords in the URL\n",
        "def contains_suspicious_keywords(url):\n",
        "    suspicious_keywords = ['login', 'verify', 'account', 'update', 'secure', 'banking', 'payment']\n",
        "    return any(keyword in url for keyword in suspicious_keywords)\n",
        "\n",
        "# Function to check for IP address in the URL\n",
        "def contains_ip_address(url):\n",
        "    pattern = r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"\n",
        "    return 1 if re.search(pattern, url) else 0\n",
        "\n",
        "# Function to extract the number of hyphens in the domain name\n",
        "def num_hyphens_in_domain(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return ext.domain.count('-')\n",
        "\n",
        "# Function to count dots in the URL\n",
        "def count_dots(url):\n",
        "    return url.count('.')\n",
        "\n",
        "# Function to check if the URL contains a shortened link (bit.ly, goo.gl, etc.)\n",
        "def is_url_shortened(url):\n",
        "    shortened_services = ['bit.ly', 'goo.gl', 'tinyurl.com']\n",
        "    return 1 if any(service in url for service in shortened_services) else 0\n",
        "\n",
        "# Function to check if URL contains the \"@\" symbol\n",
        "def contains_at_symbol(url):\n",
        "    return 1 if '@' in url else 0\n",
        "\n",
        "# Function to check if the SSL certificate is valid (optional, may require network requests)\n",
        "def ssl_cert_valid(url):\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        hostname = parsed_url.hostname\n",
        "        context = ssl.create_default_context()\n",
        "        connection = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=hostname)\n",
        "        connection.connect((hostname, 443))\n",
        "        cert = connection.getpeercert()\n",
        "        return 1 if cert else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Function to extract the TLD (Top-Level Domain)\n",
        "def get_tld(url):\n",
        "    ext = tldextract.extract(url)\n",
        "    return ext.suffix\n",
        "\n",
        "# Function to extract the number of special characters in the URL\n",
        "def count_special_characters(url):\n",
        "    return len(re.findall(r'[^A-Za-z0-9]', url))\n",
        "\n",
        "# Function to extract URL features\n",
        "def extract_url_features(url):\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"url_length\": get_url_length(url),\n",
        "        \"num_subdomains\": get_num_subdomains(url),\n",
        "        \"is_https\": is_https(url),\n",
        "        \"contains_suspicious_keywords\": contains_suspicious_keywords(url),\n",
        "        \"contains_ip_address\": contains_ip_address(url),\n",
        "        \"num_hyphens_in_domain\": num_hyphens_in_domain(url),\n",
        "        \"dot_count\": count_dots(url),\n",
        "        \"is_url_shortened\": is_url_shortened(url),\n",
        "        \"contains_at_symbol\": contains_at_symbol(url),\n",
        "        \"ssl_cert_valid\": ssl_cert_valid(url),\n",
        "        \"tld\": get_tld(url),\n",
        "        \"special_char_count\": count_special_characters(url)\n",
        "    }\n",
        "\n",
        "# Load the filtered dataset with URL and label\n",
        "file_path = 'filtered_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ask user for starting and ending index for records to process\n",
        "start_index = int(input(\"Enter the starting record index: \"))\n",
        "end_index = int(input(\"Enter the ending record index: \"))\n",
        "\n",
        "# Ensure that the ending index is within the range of the dataframe\n",
        "end_index = min(end_index, len(df))\n",
        "\n",
        "# Extract a subset of the dataset based on user inputs\n",
        "df_subset = df.iloc[start_index:end_index]\n",
        "\n",
        "# Initialize total records and counter\n",
        "total_records = len(df_subset)\n",
        "completed_records = 0\n",
        "\n",
        "# Extract features from URLs and show live progress\n",
        "url_features = []\n",
        "for _, row in df_subset.iterrows():\n",
        "    url_features.append(extract_url_features(row['URL']))\n",
        "    completed_records += 1\n",
        "    print(f\"Processing: {completed_records}/{total_records} records completed\", end='\\r')\n",
        "\n",
        "# Convert the features into a dataframe\n",
        "features_df = pd.DataFrame(url_features)\n",
        "\n",
        "# Copy the label column as is from the parent dataset\n",
        "features_df['label'] = df_subset['label'].values\n",
        "\n",
        "# Define the output file path dynamically\n",
        "output_file_name = f\"extracted_url_features_{start_index}_{end_index}.csv\"\n",
        "features_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "print(f\"\\nExtracted URL features for records {start_index} to {end_index} saved to: {output_file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeDGl4sCIVaD",
        "outputId": "cdce1016-adf2-4da1-a375-bb72f9da0102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the starting record index: 201\n",
            "Enter the ending record index: 220\n",
            "Processing: 19/19 records completed\n",
            "Extracted URL features for records 201 to 220 saved to: extracted_url_features_201_220.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "hCO9ScSxIWqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Step 1: Load datasets\n",
        "legitimate_df = pd.read_csv('structured_data_legitimate.csv')\n",
        "phishing_df = pd.read_csv('structured_data_phishing.csv')\n",
        "\n",
        "# Step 2: Combine datasets\n",
        "legitimate_df['label'] = 0  # Label 0 for legitimate sites\n",
        "phishing_df['label'] = 1  # Label 1 for phishing sites\n",
        "\n",
        "# Concatenate the datasets\n",
        "df = pd.concat([legitimate_df, phishing_df], ignore_index=True)\n",
        "\n",
        "# Step 3: Refined Feature Extraction Function (fixing URL complexity issue)\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Extract domain and path\n",
        "    domain = urlparse(url).hostname\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    # URL length\n",
        "    features['url_length'] = len(url)\n",
        "\n",
        "    # Number of subdomains\n",
        "    features['num_subdomains'] = domain.count('.') - 1\n",
        "\n",
        "    # Presence of HTTPS\n",
        "    features['uses_https'] = url.startswith('https')\n",
        "\n",
        "    # Presence of an IP address in the URL\n",
        "    features['has_ip'] = bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url))\n",
        "\n",
        "    # Presence of special characters\n",
        "    features['has_special_chars'] = bool(re.search(r'[^A-Za-z0-9./:_-]', url))\n",
        "\n",
        "    # Handle query parameters and fragments:\n",
        "    features['has_query'] = bool(urlparse(url).query)\n",
        "    features['has_fragment'] = bool(urlparse(url).fragment)\n",
        "\n",
        "    # **Handle trusted domains with query parameters or fragments leniently**\n",
        "    trusted_domains = ['google.com', 'facebook.com', 'youtube.com', 'amazon.com', 'colab.research.google.com']\n",
        "    features['is_known_domain'] = int(any(domain.endswith(known_domain) for known_domain in trusted_domains))\n",
        "\n",
        "    # If the domain is trusted, give less importance to query and fragment\n",
        "    if features['is_known_domain']:\n",
        "        features['query_fragment_penalty'] = 0  # No penalty for query/fragment in trusted domains\n",
        "    else:\n",
        "        features['query_fragment_penalty'] = features['has_query'] + features['has_fragment']\n",
        "\n",
        "    # Check for phishing-related keywords in the domain or path (login, secure, etc.)\n",
        "    phishing_keywords = ['login', 'account', 'secure', 'update', 'verify', 'confirm', 'bank']\n",
        "    features['has_phishing_keywords'] = int(any(keyword in domain or keyword in path for keyword in phishing_keywords))\n",
        "\n",
        "    # Path length\n",
        "    features['path_length'] = len(path)\n",
        "\n",
        "    # Check for suspicious domain (random characters in domain name)\n",
        "    features['has_random_chars'] = bool(re.search(r'\\.[a-z]{5,}', domain))\n",
        "\n",
        "    return features\n",
        "\n",
        "# Step 4: Apply feature extraction on all URLs\n",
        "df_features = df['URL'].apply(extract_features)\n",
        "\n",
        "# Step 5: Convert the extracted features into a DataFrame\n",
        "features_df = pd.DataFrame(df_features.tolist())\n",
        "\n",
        "# Step 6: Prepare the feature matrix (X) and target vector (y)\n",
        "X = features_df  # Features\n",
        "y = df['label']  # Target labels (0 = legitimate, 1 = phishing)\n",
        "\n",
        "# Step 7: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Feature scaling (important for models that use distance metrics)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 9: Train XGBoost Classifier\n",
        "model = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 10: Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Classification Report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Step 11: Cross-validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f'\\nCross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%')\n",
        "\n",
        "# Step 12: Dynamic URL Prediction\n",
        "def dynamic_predict_url():\n",
        "    print(\"Welcome to the URL phishing detection system.\")\n",
        "    print(\"You can enter a URL to check if it is phishing or legitimate.\")\n",
        "    print(\"To exit, type 'exit'.\")\n",
        "\n",
        "    while True:\n",
        "        # Get the URL input from the user\n",
        "        url = input(\"Enter URL: \")\n",
        "\n",
        "        # Exit condition\n",
        "        if url.lower() == 'exit':\n",
        "            print(\"Exiting the prediction system.\")\n",
        "            break\n",
        "\n",
        "        # Extract features from the URL\n",
        "        features = extract_features(url)\n",
        "        features_df = pd.DataFrame([features])\n",
        "\n",
        "        # Scale the features using the scaler\n",
        "        features_scaled = scaler.transform(features_df)\n",
        "\n",
        "        # Predict using the trained model\n",
        "        prediction = model.predict(features_scaled)\n",
        "\n",
        "        # Display the result\n",
        "        if prediction == 1:\n",
        "            print(f\"URL: {url} -> Phishing\")\n",
        "        else:\n",
        "            print(f\"URL: {url} -> Legitimate\")\n",
        "\n",
        "# Call the dynamic prediction function (uncomment the line below to run in interactive mode)\n",
        "dynamic_predict_url()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phEQZm3foHsu",
        "outputId": "f7ef036b-e2a3-47b0-bfff-24152c90ed38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.89%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      3265\n",
            "           1       1.00      1.00      1.00      2052\n",
            "\n",
            "    accuracy                           1.00      5317\n",
            "   macro avg       1.00      1.00      1.00      5317\n",
            "weighted avg       1.00      1.00      1.00      5317\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3265    0]\n",
            " [   6 2046]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [13:39:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Accuracy: 99.85%\n",
            "Welcome to the URL phishing detection system.\n",
            "You can enter a URL to check if it is phishing or legitimate.\n",
            "To exit, type 'exit'.\n",
            "Enter URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=phEQZm3foHsu\n",
            "URL: https://colab.research.google.com/drive/1gxXNmPdXW1vhXif1ztyjPGobbn7WHZDh?authuser=2#scrollTo=phEQZm3foHsu -> Phishing\n",
            "Enter URL: exit\n",
            "Exiting the prediction system.\n"
          ]
        }
      ]
    }
  ]
}